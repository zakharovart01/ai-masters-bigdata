{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c02f29e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "SPARK_HOME = \"/usr/lib/spark3\"\n",
    "PYSPARK_PYTHON = \"/opt/conda/envs/dsenv/bin/python\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]= PYSPARK_PYTHON\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]= PYSPARK_PYTHON\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "\n",
    "PYSPARK_HOME = os.path.join(SPARK_HOME, \"python/lib\")\n",
    "sys.path.insert(0, os.path.join(PYSPARK_HOME, \"py4j-0.10.9.3-src.zip\"))\n",
    "sys.path.insert(0, os.path.join(PYSPARK_HOME, \"pyspark.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964be5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up _JAVA_OPTIONS: \n",
      "Picked up _JAVA_OPTIONS: \n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark3/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.ui.port\", \"41050\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"Spark ML Intro\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f2c658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://edge1.ru-central1.internal:41050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark ML Intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9c5e1b8850>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c975b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56aa763e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 0) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 22:20:20 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/16 22:20:35 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/16 22:20:50 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 0) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 22:21:05 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/16 22:21:20 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/16 22:21:35 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/16 22:21:50 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 0) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 22:22:05 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/16 22:22:20 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/16 22:22:35 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/16 22:22:50 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 0) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 22:23:05 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/16 22:23:20 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/16 22:23:35 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/16 22:23:50 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 0) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 22:24:05 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/16 22:24:20 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/16 22:24:35 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/16 22:24:50 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = \"/datasets/amazon/all_reviews_5_core_train_small.json\"\n",
    "data = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1548f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5dca7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"wordsReview\", pattern=\"\\\\W\")\n",
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"reviewFiltered\", stopWords=stop_words)\n",
    "count_vectorizer = CountVectorizer(inputCol=swr.getOutputCol(), outputCol=\"reviewVector\", binary=True, vocabSize=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65cec4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"reviewVector\", labelCol=\"overall\",  maxIter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0efb366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"overall\", predictionCol=\"prediction\", metricName='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01bf06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,   \n",
    "    swr,    \n",
    "    count_vectorizer,\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b3b301b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 22:28:08 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/04/16 22:28:08 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/04/16 22:28:08 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/04/16 22:28:08 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bf9fc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0560346171390937"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipeline_model.transform(test)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33506917",
   "metadata": {},
   "source": [
    "## Подберем гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "143948fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 19:48:38 ERROR ContextCleaner: Error cleaning shuffle 120\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n",
      "\tat org.apache.spark.ContextCleaner.doCleanupShuffle(ContextCleaner.scala:241)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3(ContextCleaner.scala:202)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3$adapted(ContextCleaner.scala:195)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 19:52:49 ERROR BlockManagerStorageEndpoint: Error in removing shuffle 190\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/16 19:52:49 WARN BlockManagerMaster: Failed to remove shuffle 190 - null\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 19:56:53 ERROR ContextCleaner: Error cleaning shuffle 274\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n",
      "\tat org.apache.spark.ContextCleaner.doCleanupShuffle(ContextCleaner.scala:241)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3(ContextCleaner.scala:202)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3$adapted(ContextCleaner.scala:195)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 19:57:43 ERROR ContextCleaner: Error cleaning shuffle 279\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n",
      "\tat org.apache.spark.ContextCleaner.doCleanupShuffle(ContextCleaner.scala:241)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3(ContextCleaner.scala:202)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3$adapted(ContextCleaner.scala:195)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 20:00:30 ERROR BlockManagerStorageEndpoint: Error in removing shuffle 332\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/16 20:00:30 WARN BlockManagerMaster: Failed to remove shuffle 332 - null\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 20:00:36 ERROR BlockManagerStorageEndpoint: Error in removing shuffle 341\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/16 20:00:36 WARN BlockManagerMaster: Failed to remove shuffle 341 - null\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 20:03:13 ERROR BlockManagerStorageEndpoint: Error in removing shuffle 363\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/16 20:03:13 WARN BlockManagerMaster: Failed to remove shuffle 363 - null\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0, 2, 5]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9efdeb19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.057185548973339, 1.502276762446981, 1.5173387321188316]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "158f8a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"reviewVector\", labelCol=\"overall\", regParam=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0190f86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 20:04:10 ERROR BlockManagerStorageEndpoint: Error in removing shuffle 389\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/16 20:04:10 WARN BlockManagerMaster: Failed to remove shuffle 389 - null\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 20:05:14 ERROR BlockManagerStorageEndpoint: Error in removing shuffle 412\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/16 20:05:14 WARN BlockManagerMaster: Failed to remove shuffle 412 - null\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 20:12:42 ERROR ContextCleaner: Error cleaning shuffle 548\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n",
      "\tat org.apache.spark.ContextCleaner.doCleanupShuffle(ContextCleaner.scala:241)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3(ContextCleaner.scala:202)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3$adapted(ContextCleaner.scala:195)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 20:18:29 ERROR ContextCleaner: Error cleaning shuffle 662\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n",
      "\tat org.apache.spark.ContextCleaner.doCleanupShuffle(ContextCleaner.scala:241)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3(ContextCleaner.scala:202)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3$adapted(ContextCleaner.scala:195)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.maxIter, [5, 10, 30]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bc19090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0574254548582926, 1.057185548973339, 1.057185548973339]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df3ff97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"reviewVector\", labelCol=\"overall\", regParam=0)\n",
    "count_vectorizer = CountVectorizer(inputCol=swr.getOutputCol(), outputCol=\"reviewVector\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e5c1890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(count_vectorizer.vocabSize, [2000, 2500, 3000])\\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0d4cdfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0574254548582926, 1.0574254548582926, 1.0574062317777269]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f4daa67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='CountVectorizer_933773031e9d', name='vocabSize', doc='max size of the vocabulary. Default 1 << 18.'): 2000},\n",
       " {Param(parent='CountVectorizer_933773031e9d', name='vocabSize', doc='max size of the vocabulary. Default 1 << 18.'): 2500},\n",
       " {Param(parent='CountVectorizer_933773031e9d', name='vocabSize', doc='max size of the vocabulary. Default 1 << 18.'): 3000}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9723194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"reviewVector\", labelCol=\"overall\", maxIter=10, regParam=0)\n",
    "count_vectorizer = CountVectorizer(inputCol=swr.getOutputCol(), outputCol=\"reviewVector\", binary=True, vocabSize=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df858a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[count_vectorizer.getOutputCol(), 'verified'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8e91bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,   \n",
    "    swr,    \n",
    "    count_vectorizer,\n",
    "    assembler,\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02d08b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 296 ms, sys: 69.2 ms, total: 365 ms\n",
      "Wall time: 3min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f67248e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5077387925964552"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipeline_model.transform(test)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da809292",
   "metadata": {},
   "source": [
    "Следовательно, выбираем maxIter=10, regParam=0, vocabSize=2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d355a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsenv",
   "language": "python",
   "name": "dsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
